{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from keras.callbacks import Callback\n",
    "# from keras import backend as K\n",
    "# import pandas as pd\n",
    "\n",
    "#class_weight={0:3.3359,1:0.3368,2:3.0813,3:2.7868,4:0.7300,5:1.4757}\n",
    "\n",
    "\n",
    "      ###### প্রতি টা ক্লাসের ওয়েইট ক্যাল্কুলেট হচ্ছে এখানে।  #############\n",
    "def compute_weight(Y, classes):\n",
    "    num_samples = len(Y)\n",
    "    n_classes = len(classes)\n",
    "    num_bin = np.bincount(Y[:, 0])\n",
    "    class_weights = {i: (num_samples / (n_classes * num_bin[i])) for i in range(6)}\n",
    "    return class_weights\n",
    "\n",
    "\n",
    "def patientSplitter(randomIDfile,df2,split_portion):\n",
    "    import pandas as pd\n",
    "    \n",
    "    df1 = pd.read_csv(randomIDfile,header=None)\n",
    "    split_portion_numer=int(split_portion*61)\n",
    "    \n",
    "    train_pat_list = [int(each) for each in df1.iloc[:split_portion_numer].values]\n",
    "    test_pat_list = [int(each) for each in df1.iloc[split_portion_numer:].values]\n",
    "    print(test_pat_list)\n",
    "    df3 = []\n",
    "    df4 = []\n",
    "    for pat_ID in train_pat_list:\n",
    "        df3.append(df2[df2.patID == pat_ID].values)\n",
    "        print(pat_ID)\n",
    "    for pat_ID in test_pat_list:\n",
    "        df4.append(df2[df2.patID == pat_ID].values) \n",
    "        print(pat_ID)\n",
    "    del df2\n",
    "    df3 = pd.np.vstack(df3)\n",
    "    df4 = pd.np.vstack(df4)\n",
    "    X_train = df3[0:, :3000]\n",
    "    X_test= df4[0:, :3000]\n",
    "    Y_train= df3[0:,3000:3001]\n",
    "    Y_test= df4[0:,3000:3001]\n",
    "    pat_train=df3[0:, 3002:3003]\n",
    "    pat_test= df4[0:, 3002:3003]\n",
    "    \n",
    "    del pd\n",
    "    return X_train,X_test,Y_train,Y_test,pat_train,pat_test\t\n",
    "\n",
    "\n",
    "    #### রেজাল্ট এর লগিং এখানে। একটা কল ব্যাক দিয়ে সিএসভি ফাইলে গিয়ে ডাম্প করে আসতে হবে।\n",
    "def results_log(filepath, params):\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    df1 = pd.read_csv() #training.csv\n",
    "\n",
    "    #logging pipeline here ##\n",
    "\n",
    "    #print()\n",
    "\n",
    "\n",
    "    ########## লগ ম্যাট্রিক্স বানানো।  -_-  #################\n",
    "class log_metrics():\n",
    "    '''Custom callback to add new elements to epoch end logs and tensorflow graphs\n",
    "\n",
    "        # Callback called on_epoch_end to calcualte validations set metrics using meta\n",
    "        # information (Patient ID, channel ID, etc). End of epoch metrics are added to\n",
    "        # `logs` which are later used by the tensorboard callback.\n",
    "        # log_metrics should be specified in the callback list before tensorboard callback\n",
    "        # '''\n",
    "    \n",
    "\n",
    "    ###### লগ ম্যাট্রিক্স ক্লাসের ইনিশিয়ালাইজার ##################\n",
    "    #### চার টা জিনিস নিচ্ছে সে। এক্স এর মান, ওয়াই এর মান, পেশেন্ট আইডি। আর কাজ করানোর জন্য খরগোশ।\n",
    "    def __init__(self, valX, valY, patID, **kwargs):\n",
    "        self.valY = valY\n",
    "        self.valX = valX\n",
    "        self.patID = patID\n",
    "        super(log_metrics,self).__init__(**kwargs) #### এই সুপার টা এই ফাংশনের পার্ট না হয়ে মেইন ক্লাসের পার্ট হয়ে গেল। \n",
    "\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "\n",
    "        if logs is not None:\n",
    "            predY = self.model.predict(self.valX, verbose=0)\n",
    "            predY = np.argmax(predY, axis=-1)\n",
    "\n",
    "            # Enter metric calculations per patient here\n",
    "            #\n",
    "\n",
    "            ### Learning Rate logging for Adam ###\n",
    "\n",
    "            lr = self.model.optimizer.lr\n",
    "            if self.model.optimizer.initial_decay > 0:\n",
    "                lr *= (1. / (1. + self.model.optimizer.decay * K.cast(self.model.optimizer.iterations, K.dtype(self.model.optimizer.decay))))\n",
    "            t = K.cast(self.model.optimizer.iterations, K.floatx()) + 1\n",
    "            lr_t = lr * (K.sqrt(1. - K.pow(self.model.optimizer.beta_2, t)) / (1. - K.pow(self.model.optimizer.beta_1, t)))\n",
    "            logs['lr'] = np.array(float(K.get_value(lr_t)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\thesis\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, Activation, add, Dropout, Input\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import initializers\n",
    "from keras.engine import Layer, InputSpec\n",
    "#from keras.engine import input_layer\n",
    "from keras import backend as K\n",
    "from keras.constraints import max_norm\n",
    "#from input_layer import Input\n",
    "\n",
    "class Scale(Layer):\n",
    "    '''Custom Layer for ResNet used for BatchNormalization.\n",
    "\n",
    "    Learns a set of weights and biases used for scaling the input data.\n",
    "    the output consists simply in an element-wise multiplication of the input\n",
    "    and a sum of a set of constants:\n",
    "        out = in * gamma + beta,\n",
    "    where 'gamma' and 'beta' are the weights and biases larned.\n",
    "    # Arguments\n",
    "        axis: integer, axis along which to normalize in mode 0. For instance,\n",
    "            if your input tensor has shape (samples, channels, rows, cols),\n",
    "            set axis to 1 to normalize per feature map (channels axis).\n",
    "        momentum: momentum in the computation of the\n",
    "            exponential average of the mean and standard deviation\n",
    "            of the data, for feature-wise normalization.\n",
    "        weights: Initialization weights.\n",
    "            List of 2 Numpy arrays, with shapes:\n",
    "            `[(input_shape,), (input_shape,)]`\n",
    "        beta_init: name of initialization function for shift parameter\n",
    "            (see [initializers](../initializers.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        gamma_init: name of initialization function for scale parameter (see\n",
    "            [initializers](../initializers.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, weights=None, axis=-1, momentum=0.9, beta_init='he_normal', gamma_init='he_normal', **kwargs):\n",
    "        self.momentum = momentum\n",
    "        self.axis = axis\n",
    "        self.beta_init = initializers.get(beta_init)\n",
    "        self.gamma_init = initializers.get(gamma_init)\n",
    "        self.initial_weights = weights\n",
    "        super(Scale, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        shape = (int(input_shape[self.axis]),)\n",
    "\n",
    "        self.gamma = K.variable(self.gamma_init(shape), name='%s_gamma' % self.name)\n",
    "        self.beta = K.variable(self.beta_init(shape), name='%s_beta' % self.name)\n",
    "        self.trainable_weights = [self.gamma, self.beta]\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        input_shape = self.input_spec[0].shape\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "\n",
    "        out = K.reshape(self.gamma, broadcast_shape) * x + K.reshape(self.beta, broadcast_shape)\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"momentum\": self.momentum, \"axis\": self.axis}\n",
    "        base_config = super(Scale, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "def res_subsam(input_tensor, filters=(64,64), kernel_size=16, subsam=2, dropout_rate=0.2, bias=False, maxnorm=4., **kwargs):\n",
    "    eps = 1.1e-5\n",
    "    nb_filter1, nb_filter2 = filters\n",
    "    x = BatchNormalization(epsilon=eps, axis=-1)(input_tensor)\n",
    "    x = Scale(axis=-1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(rate=dropout_rate, seed=1)(x)\n",
    "    x = Conv1D(filters=nb_filter1, kernel_initializer=initializers.he_normal(seed=1), kernel_size=kernel_size,\n",
    "               padding='same', use_bias=bias, kernel_constraint=max_norm(maxnorm))(x)  ##\n",
    "    x = MaxPooling1D(pool_size=subsam)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=-1)(x)\n",
    "    x = Scale(axis=-1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(rate=dropout_rate, seed=1)(x)\n",
    "    x = Conv1D(filters=nb_filter2, kernel_initializer=initializers.he_normal(seed=1), kernel_size=kernel_size,\n",
    "               padding='same', use_bias=bias, kernel_constraint=max_norm(maxnorm))(x)  ##\n",
    "    short = Conv1D(filters=nb_filter2, kernel_size=kernel_size, padding='same', use_bias=bias,\n",
    "                   kernel_constraint=max_norm(maxnorm), kernel_initializer=initializers.he_normal(seed=1))(\n",
    "        input_tensor)  ##\n",
    "    short = MaxPooling1D(pool_size=subsam)(short)\n",
    "    x = add([x, short])\n",
    "    return x\n",
    "\n",
    "\n",
    "def res_nosub(input_tensor, filters=(64,64), kernel_size=16, dropout_rate=0.2, bias=False, maxnorm=4., **kwargs):\n",
    "    eps = 1.1e-5\n",
    "    nb_filter1, nb_filter2 = filters\n",
    "    x = BatchNormalization(epsilon=eps, axis=-1)(input_tensor)\n",
    "    x = Scale(axis=-1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(rate=dropout_rate, seed=1)(x)\n",
    "    x = Conv1D(filters=nb_filter1, kernel_initializer=initializers.he_normal(seed=1), kernel_size=kernel_size,\n",
    "               padding='same', use_bias=bias, kernel_constraint=max_norm(maxnorm))(x)  ##\n",
    "    x = BatchNormalization(epsilon=eps, axis=-1)(x)\n",
    "    x = Scale(axis=-1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(rate=dropout_rate, seed=1)(x)\n",
    "    x = Conv1D(filters=nb_filter2, kernel_initializer=initializers.he_normal(seed=1), kernel_size=kernel_size,\n",
    "               padding='same', use_bias=bias, kernel_constraint=max_norm(maxnorm))(x)  ##\n",
    "    x = add([x, input_tensor])\n",
    "    return x\n",
    "\n",
    "\n",
    "def res_first(input_tensor, filters=(64,64), kernel_size=16, dropout_rate=0.2, bias=False, maxnorm=4., **kwargs):\n",
    "    eps = 1.1e-5\n",
    "    nb_filter1, nb_filter2 = filters\n",
    "    x = Conv1D(filters=nb_filter1, kernel_initializer=initializers.he_normal(seed=1), kernel_size=kernel_size,\n",
    "               padding='same', use_bias=bias, kernel_constraint=max_norm(maxnorm))(input_tensor)  ##\n",
    "    x = BatchNormalization(epsilon=eps, axis=-1)(x)\n",
    "    x = Scale(axis=-1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(rate=dropout_rate, seed=1)(x)\n",
    "    x = Conv1D(filters=nb_filter2, kernel_initializer=initializers.he_normal(seed=1), kernel_size=kernel_size,\n",
    "               padding='same', use_bias=bias, kernel_constraint=max_norm(maxnorm))(x)  ##\n",
    "    x = add([x, input_tensor])\n",
    "    return x\n",
    "\n",
    "\n",
    "def eegnet(eeg_length=3000, kernel_size=16, bias=False, maxnorm=4., **kwargs):\n",
    "\n",
    "    '''\n",
    "    Top model for the CNN\n",
    "    Add details of module in docstring\n",
    "        '''\n",
    "\n",
    "    eps = 1.1e-5\n",
    "\n",
    "\n",
    "    EEG_input = Input(shape=(eeg_length,1))\n",
    "    x = Conv1D(filters=64, kernel_size=kernel_size, kernel_initializer=initializers.he_normal(seed=1), padding='same',\n",
    "               use_bias=bias, kernel_constraint=max_norm(maxnorm))(EEG_input)  ##\n",
    "    x = BatchNormalization(epsilon=eps, axis=-1)(x)\n",
    "    x = Scale(axis=-1)(x)\n",
    "    x = Activation('relu')(x)  # ব্যাচ নরমালাইজ করা ভার্শন টা কে নিয়ে স্কেলের মধ্যে ঢুকানো \n",
    "\n",
    "    x = res_first(x, filters=[64, 64], kernel_size=kernel_size)\n",
    "    x = res_subsam(x, filters=[64, 64], kernel_size=kernel_size, subsam=2)\n",
    "    x = res_nosub(x, filters=[64, 64], kernel_size=kernel_size)\n",
    "    x = res_subsam(x, filters=[64, 128], kernel_size=kernel_size, subsam=2)\n",
    "    x = res_nosub(x, filters=[128, 128], kernel_size=kernel_size)\n",
    "    x = res_subsam(x, filters=[128, 128], kernel_size=kernel_size, subsam=2)\n",
    "    x = res_nosub(x, filters=[128, 128], kernel_size=kernel_size)\n",
    "    x = res_subsam(x, filters=[128, 192], kernel_size=kernel_size, subsam=2)\n",
    "    x = res_nosub(x, filters=[192, 192], kernel_size=kernel_size)\n",
    "    x = res_subsam(x, filters=[192, 192], kernel_size=kernel_size, subsam=2)\n",
    "    x = res_nosub(x, filters=[192, 192], kernel_size=kernel_size)\n",
    "    x = res_subsam(x, filters=[192, 256], kernel_size=kernel_size, subsam=2)\n",
    "    x = res_nosub(x, filters=[256, 256], kernel_size=kernel_size)\n",
    "    x = res_subsam(x, filters=[256, 256], kernel_size=kernel_size, subsam=2)\n",
    "    x = res_nosub(x, filters=[256, 256], kernel_size=kernel_size)\n",
    "    x = res_subsam(x, filters=[256, 512], kernel_size=kernel_size, subsam=2)\n",
    "    x = BatchNormalization(epsilon=eps, axis=-1)(x)\n",
    "    x = Scale(axis=-1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--seed SEED] [--loadmodel LOADMODEL]\n",
      "                             [--epochs EPOCHS] [--batch_size BATCH_SIZE]\n",
      "                             [--verbose {1,2}] [--classweights CLASSWEIGHTS]\n",
      "                             [--comment COMMENT]\n",
      "                             fold\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\envs\\thesis\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, absolute_import, division\n",
    "import tensorflow as tf\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "# set_session(tf.Session(config=config))\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import os\n",
    "import tables\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adamax as opt\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger\n",
    "import pandas as pd\n",
    "\n",
    "from modules import *\n",
    "from utils import *\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "\t############ পারসার এই ক্যাচাল এইখান থেকে শুরু #######################\n",
    "\t\n",
    "\t\n",
    "\t########## পারসার ডেফিনিশন ########\n",
    "\tparser = argparse.ArgumentParser(description='')\n",
    "\tparser.add_argument(\"fold\",\n",
    "\t\t\t\t\t\thelp=\"csvfile to use\")\n",
    "\tparser.add_argument(\"--seed\", type=int,\n",
    "\t\t\t\t\t\thelp=\"Random seed\")\n",
    "\tparser.add_argument(\"--loadmodel\",\n",
    "\t\t\t\t\t\thelp=\"load previous model checkpoint for retraining (Enter absolute path)\")\n",
    "\tparser.add_argument(\"--epochs\", type=int,\n",
    "\t\t\t\t\t\thelp=\"Number of epochs for training\")\n",
    "\tparser.add_argument(\"--batch_size\", type=int,\n",
    "\t\t\t\t\t\thelp=\"number of minibatches to take during each backwardpass preferably multiple of 2\")\n",
    "\tparser.add_argument(\"--verbose\", type=int, choices=[1, 2],\n",
    "\t\t\t\t\t\thelp=\"Verbosity mode. 1 = progress bar, 2 = one line per epoch (default 2)\")\n",
    "\tparser.add_argument(\"--classweights\", type=bool,\n",
    "\t\t\t\t\t\thelp=\"if True, class weights are added\")\n",
    "\tparser.add_argument(\"--comment\",\n",
    "\t\t\t\t\t\thelp=\"Add comments to the log files\")\n",
    "\n",
    "\targs = parser.parse_args()\n",
    "\tprint(\"%s selected\" % (args.fold))\n",
    "\tfoldname = args.fold\n",
    "\n",
    "\t###### পারসার থেকে নিয়ে ভ্যারিয়েবল গুলাতে মান বসানো, মান না থাকলে ডিফল্ট কত হবে সেইগুলাও বসান ########\n",
    "\t\n",
    "\tif args.seed:  # if random seed is specified\n",
    "\t\tprint(\"Random seed specified as %d\" % (args.seed))\n",
    "\t\trandom_seed = args.seed\n",
    "\telse:\n",
    "\t\trandom_seed = 1\n",
    "\n",
    "\tif args.loadmodel:  # If a previously trained model is loaded for retraining\n",
    "\t\tload_path = args.loadmodel  #### path to model to be loaded\n",
    "\n",
    "\t\tidx = load_path.find(\"weights\")\n",
    "\t\tinitial_epoch = int(load_path[idx + 8:idx + 8 + 4])\n",
    "\n",
    "\t\tprint(\"%s model loaded\\nInitial epoch is %d\" % (args.loadmodel, initial_epoch))\n",
    "\telse:\n",
    "\t\tprint(\"no model specified, using initializer to initialize weights\")\n",
    "\t\tinitial_epoch = 0\n",
    "\t\tload_path = False\n",
    "\n",
    "\tif args.epochs:  # if number of training epochs is specified\n",
    "\t\tprint(\"Training for %d epochs\" % (args.epochs))\n",
    "\t\tepochs = args.epochs\n",
    "\telse:\n",
    "\t\tepochs = 200\n",
    "\t\tprint(\"Training for %d epochs\" % (epochs))\n",
    "\n",
    "\tif args.batch_size:  # if batch_size is specified\n",
    "\t\tprint(\"Training with %d samples per minibatch\" % (args.batch_size))\n",
    "\t\tbatch_size = args.batch_size\n",
    "\telse:\n",
    "\t\tbatch_size = 1024\n",
    "\t\tprint(\"Training with %d minibatches\" % (batch_size))\n",
    "\n",
    "\n",
    "\tif args.verbose:\n",
    "\t\t\tverbose = args.verbose\n",
    "\t\t\tprint(\"Verbosity level %d\" % (verbose))\n",
    "\telse:\n",
    "\t\tverbose = 2\n",
    "\tif args.comment:\n",
    "\t\t\tcomment = args.comment\n",
    "\telse:\n",
    "\t\tcomment = None\n",
    "\t\n",
    "\t\n",
    "\t######## এই পর্যন্ত শুধু পারসার থেকে মান নিয়ে ভ্যারিয়েবল গুলা তে বসেছে। ###############\n",
    "\t\n",
    "\t### ডিরেক্টরি ডিফাইন করা জেনারেলাইজ করে ####\n",
    "\t\n",
    "\tmodel_dir = os.path.join(os.getcwd(),'..','models').replace('\\\\','/')\n",
    "\tfold_dir = os.path.join(os.getcwd(),'..','data').replace('\\\\','/')\n",
    "\tlog_dir = os.path.join(os.getcwd(),'..','logs').replace('\\\\','/')\n",
    "\tlog_name = foldname + '_' + str(datetime.now()).replace(' ','').replace('\\\\','/')\n",
    "\tprint(os.path.join(model_dir,log_name))\n",
    "\tif not os.path.exists(os.path.join(model_dir,log_name)):\n",
    "\t\tnew_dir =(os.path.join(os.getcwd(),'..','models',log_name)).replace('\\\\','/').replace(':','')\n",
    "\t\tprint(new_dir)\n",
    "\t\tos.makedirs(new_dir)\n",
    "\tcheckpoint_name = os.path.join(model_dir,log_name,'weights.{epoch04d}-{val_acc.4f}.hdf5'.replace(':','')) # make sure separate\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# folder for each log_name\n",
    "\tresults_file = os.path.join(os.getcwd().replace('\\\\','/'), '..', 'results.csv')\n",
    "\t##### ডিরেক্টরি ক্যাচাল শেষ #####\n",
    "\t\n",
    "\t\n",
    "\t##### প্যারামস হচ্ছে একটা লিস্ট যেটা নেটওয়ার্ক কে খাওয়াতে হবে। এই লিস্টে সব হাইপারপ্যারামিটার থেকে শুরু করে ফোল্ডারের নাম সব থাকবে। এটা শিখলাম। কাজ করানো টা শিখতে হবে ################\n",
    "\t\n",
    "\tparams={               # still not universal\n",
    "\n",
    "\t\t'num_classes':7,  ### automate; number of classes depends on data fold\n",
    "\t\t'batch_size':batch_size,\n",
    "\t\t'epochs':epochs,\n",
    "\t\t'foldname':foldname,\n",
    "\t\t'random_seed':random_seed,\n",
    "\t\t'load_path':load_path,\n",
    "\t\t'shuffle':True,\n",
    "\t\t'initial_epoch':initial_epoch,\n",
    "\t\t'verbose':verbose,\n",
    "\t\t'eeg_length':3000,\n",
    "\t\t'kernel_size':16,\n",
    "\t\t'bias':True,\n",
    "\t\t'maxnorm':4.,\n",
    "\t\t'dropout_rate':0.5,\n",
    "\t\t'dropout_rate_dense':0.,\n",
    "\t\t'padding':'valid',\n",
    "\t\t'activation_function':'relu',\n",
    "\t\t'subsam':2,\n",
    "\t\t'trainable':True,\n",
    "\t\t'lr':.0001,\n",
    "\t\t'lr_decay':1e-5,\n",
    "\n",
    "\t}\n",
    "\n",
    "\t########### Data Prep ################\n",
    "\n",
    "\t# mat_cont = tables.open_file(os.path.join(fold_dir,foldname))\n",
    "\n",
    "\t#Elegant one\n",
    "\t#df2 = pd.read_csv( (os.path.join(fold_dir,foldname),header=None)\n",
    "\t#কামলা কাউন্টারপার্ট\n",
    "\tdf2 = pd.read_csv('E:/SleepWell/ASSC-master/data/purifiedallDataChannel2.csv',header=None)\n",
    "\tdf2.rename({3000: 'hyp', 3001: 'epoch', 3002: 'patID'}, axis=\"columns\", inplace=True)\n",
    "\n",
    "\n",
    "\t#X = mat_cont['data']\n",
    "\t#Y = mat_cont['hyp']\n",
    "\t#patID = mat_cont['patID']\n",
    "\n",
    "\t\n",
    "\t#trainX, valX, trainY, valY = train_test_split(X, Y, test_size=0.2, random_state=random_seed) # patID split koro bacha\n",
    "\ttrainX, valX, trainY, valY, pat_train, pat_val = patientSplitter('randomizedIDs.csv', df2, 0.7)\n",
    "\t\n",
    "\t#####এই স্প্লিটিং করা যাবে না। লিখতে হবে। পেশেন্ট আইডি এর উপর বেইজ করে স্প্লিট করতে হবে। \n",
    "\t\n",
    "\t\n",
    "\t######পেশেন্ট আইডী বেজ করে স্প্লিট করা নিয়ে কাজ করতে হবে এইখানে। ১৫ জনের ডাটা যাবে ট্রেনিং এ, বাকি দের ডেটা যাবে ভ্যালিডেশনে। ##############\n",
    "\ttrainY = to_categorical(trainY, params['num_classes'])\n",
    "\tvalY = to_categorical(valY, params['num_classes'])\n",
    "\n",
    "\t########### Create Model ################\n",
    "\t###### মডেলের শুরুর দিকের লেয়ার গুলা মডুলস নামের পাই ফাইলে আছে। এখানে শুধু ডেন্স লেয়ার গুলা আলাদা করে জইন করা হচ্ছে, কারণ এইগুলা তেই চেঞ্জ আসবে।  ######\n",
    "\t##### হাইপারপ্যারামিটার গুলা ট্রেইন করার জন্য শুধু হাইপার প্যারামিটার গুলা সম্ভলিত অংশ আলাদা করে লেখা হচ্ছে। ##################\n",
    "\t#শিখলাম ব্যপারটা। #\n",
    "\t\n",
    "\t#প্রথমে ইইজি নেট টা কে তৈরি করা, সব প্যারামিটার তাকে বলে দিয়ে ###\n",
    "\ttop_model = eegnet(**params)  # might have bugs; sub modules need kwargs integration\n",
    "\t#    top_model = eegnet(params)\n",
    "\t\n",
    "\t# এর পরে সেটার আউটপুট কে ফ্ল্যাটেন করা, যাতে করে ডেন্স লেয়ার এড করা যায়।  ফ্ল্যাটেন করার পরে প্রথম ডেন্স লেয়ার এড করা। আরো বেশি ডেন্স লেয়ার এড করা যেতে পারে ব্যপার টা তে। পরে চেষ্টা করে দেখতে হবে বিভিন্ন কনফিগারেশন।   ####\n",
    "\t#এখানে যেমন প্রথম ডেন্স লেয়ারের সাথেই সফটম্যাক্স করে আঊটপুট দেওয়া। এমন না করে আরেকটা ডেন্স লেয়ার রেখে সেইটা তে সফট্ম্যাএক্স করা উচিত। রান দেওার পরে সেই কাজ করতে হবে ###\n",
    "\tx = Flatten()(top_model)\n",
    "\tx = Dense(params['num_classes'], activation='softmax', kernel_initializer=initializers.he_normal(seed=random_seed),\n",
    "\t\t\t  kernel_constraint=max_norm(params['maxnorm']), use_bias=True)(x)  ##\n",
    "\n",
    "\tmodel = Model(top_model.Input, x) # এখানে দুইটা মডেল জোড়া লেগে যাচ্ছে। টপ মডেল, আর পরের ডেন্স করার পরের অংশ - এই দুইটা।\n",
    "\tmodel.summary() # মডেলের সামারি\n",
    "\tif load_path:  # If path for loading model was specified\n",
    "\t\tmodel.load_weights(filepath=load_path, by_name=False)\n",
    "\tplot_model(model, to_file='model.png', show_shapes=True) #  মডেল কে ইমেজ ফাইলে আঁকা\n",
    "\tmodel_json = model.to_json() #জেসন ফাইলে লেখা হচ্ছে মডেল টা কে। সব ধরনের প্রিকশন নিয়ে রাখা, আর কি।\n",
    "\twith open(os.path.join(model_dir,log_name,'model.json'), \"w\") as json_file:\n",
    "\t\tjson_file.write(model_json)\n",
    "\tmodel.compile(optimizer=opt(**params), loss='categorical_crossentropy', metrics=['accuracy']) # মডেল কম্পাইলেশন। টেক্সটবুক আচরণ, অবশেষে\n",
    "\n",
    "\t####### Define Callbacks #######\n",
    "\t\n",
    "\t### ভ্যালিডেশন একুরেসির উপর বেজ করে চেজপয়েন্ট নিয়ে রাখা মডেল সেভ করার জন্য #########\n",
    "\tmodelcheckpnt = ModelCheckpoint(filepath=checkpoint_name,\n",
    "\t\t\t\t\t\t\t\t\tmonitor='val_acc', save_best_only=False, mode='max')\n",
    "\t### টেন্সরবোরড ইন্সট্যান্স কল করা ######\t\t\t\t\t\t\t\t\n",
    "\ttensbd = TensorBoard(log_dir=os.path.join(log_dir,log_name),\n",
    "\t\t\t\t\t\t batch_size=batch_size, histogram_freq=3,\n",
    "\t\t\t\t\t\t write_grads=True,\n",
    "\t\t\t\t\t\t # embeddings_freq=99,\n",
    "\t\t\t\t\t\t # embeddings_layer_names=embedding_layer_names,\n",
    "\t\t\t\t\t\t # embeddings_data=x_val,\n",
    "\t\t\t\t\t\t # embeddings_metadata=metadata_file,\n",
    "\t\t\t\t\t\t write_images=False)\n",
    "\t\t\t\t\t\t \n",
    "\t##### সিএসভি লগারের ইন্সট্যান্স তৈরি করা, লগ সেইভ করার জন্য ###########\t\t\t\t\t \n",
    "\tcsv_logger = CSVLogger(os.path.join(log_dir,log_name,'training.csv'))\n",
    "\n",
    "\t\n",
    "\t\n",
    "\tif args.classweights:\n",
    "\t\tparams['class_weight'] = compute_weight(trainY, np.unique(trainY))\n",
    "\telse:\n",
    "\t\tparams['class_weight'] = dict(zip(np.r_[0:params['num_classes']],np.ones(params['num_classes']))) # weighted 1\n",
    "\n",
    "\t####### Train #######\n",
    "\n",
    "\ttry:\n",
    "\n",
    "\t\tmodel.fit(trainX, trainY,\n",
    "\t\t\t\tverbose=2,\n",
    "\t\t\t\tvalidation_data=(valX, valY),\n",
    "\t\t\t\tcallbacks=[modelcheckpnt,\n",
    "\t\t\t\t\t\t log_metrics(valX, valY, patID),\n",
    "\t\t\t\t\t\t tensbd, csv_logger],\n",
    "\t\t\t\t  **params)  # might have bugs\n",
    "\t\tresults_log(results_file,params)\n",
    "\n",
    "\texcept KeyboardInterrupt:\n",
    "\n",
    "\t\tresults_log(results_file, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/_2018-12-1513:37:59.858444\n",
      "E/_2018-12-15133759.858444\n"
     ]
    }
   ],
   "source": [
    "\tmodel_dir = os.path.join(os.getcwd(),'..','models').replace('\\\\','/')\n",
    "\tfold_dir = os.path.join(os.getcwd(),'..','data').replace('\\\\','/')\n",
    "\tlog_dir = os.path.join(os.getcwd(),'..','logs').replace('\\\\','/')\n",
    "\tlog_name = foldname + '_' + str(datetime.now()).replace(' ','').replace('\\\\','/')\n",
    "\tprint(os.path.join(model_dir,log_name))\n",
    "\tif not os.path.exists(os.path.join(model_dir,log_name)):\n",
    "\t\tnew_dir =(os.path.join(os.getcwd(),'..','models',log_name)).replace('\\\\','/').replace(':','')\n",
    "\t\tprint(new_dir)\n",
    "\t\tos.makedirs(new_dir)\n",
    "\tcheckpoint_name = os.path.join(model_dir,log_name,'weights.{epoch04d}-{val_acc.4f}.hdf5'.replace(':','')) # make sure separate\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# folder for each log_name\n",
    "\tresults_file = os.path.join(os.getcwd().replace('\\\\','/'), '..', 'results.csv')\n",
    "\t##### ডিরেক্টরি ক্যাচাল শেষ #####\n",
    "\t\n",
    "\t\n",
    "\t##### প্যারামস হচ্ছে একটা লিস্ট যেটা নেটওয়ার্ক কে খাওয়াতে হবে। এই লিস্টে সব হাইপারপ্যারামিটার থেকে শুরু করে ফোল্ডারের নাম সব থাকবে। এটা শিখলাম। কাজ করানো টা শিখতে হবে ################\n",
    "\t\n",
    "\tparams={               # still not universal\n",
    "\n",
    "\t\t'num_classes':6,  ### automate; number of classes depends on data fold\n",
    "\t\t'batch_size':1,\n",
    "\t\t'epochs':1,\n",
    "\t\t'foldname':\"\",\n",
    "\t\t'random_seed':7,\n",
    "\t\t'load_path':load_path,\n",
    "\t\t'shuffle':True,\n",
    "\t\t'initial_epoch':1,\n",
    "\t\t'verbose':1,\n",
    "\t\t'eeg_length':3000,\n",
    "\t\t'kernel_size':16,\n",
    "\t\t'bias':True,\n",
    "\t\t'maxnorm':4.,\n",
    "\t\t'dropout_rate':0.5,\n",
    "\t\t'dropout_rate_dense':0.,\n",
    "\t\t'padding':'valid',\n",
    "\t\t'activation_function':'relu',\n",
    "\t\t'subsam':2,\n",
    "\t\t'trainable':True,\n",
    "\t\t'lr':.0001,\n",
    "\t\t'lr_decay':1e-5,\n",
    "\n",
    "\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "foldname= \"E:/\"\n",
    "load_path = foldname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36, 20, 44, 60, 59, 28, 14, 30, 11, 15, 29, 8, 48, 24, 40, 54, 16, 37, 10, 39, 7, 18, 42, 57, 31]\n",
      "35\n",
      "21\n",
      "22\n",
      "43\n",
      "9\n",
      "56\n",
      "34\n",
      "58\n",
      "26\n",
      "6\n",
      "3\n",
      "55\n",
      "33\n",
      "2\n",
      "27\n",
      "13\n",
      "46\n",
      "45\n",
      "38\n",
      "47\n",
      "17\n",
      "5\n",
      "25\n",
      "32\n",
      "1\n",
      "52\n",
      "50\n",
      "12\n",
      "41\n",
      "51\n",
      "4\n",
      "19\n",
      "0\n",
      "23\n",
      "53\n",
      "49\n",
      "36\n",
      "20\n",
      "44\n",
      "60\n",
      "59\n",
      "28\n",
      "14\n",
      "30\n",
      "11\n",
      "15\n",
      "29\n",
      "8\n",
      "48\n",
      "24\n",
      "40\n",
      "54\n",
      "16\n",
      "37\n",
      "10\n",
      "39\n",
      "7\n",
      "18\n",
      "42\n",
      "57\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def patientSplitter(randomIDfile,df2,split_portion):\n",
    "    import pandas as pd\n",
    "    \n",
    "    df1 = pd.read_csv(randomIDfile,header=None)\n",
    "    split_portion_numer=int(split_portion*61)\n",
    "    \n",
    "    train_pat_list = [int(each) for each in df1.iloc[:split_portion_numer].values]\n",
    "    test_pat_list = [int(each) for each in df1.iloc[split_portion_numer:].values]\n",
    "    print(test_pat_list)\n",
    "    df3 = []\n",
    "    df4 = []\n",
    "    for pat_ID in train_pat_list:\n",
    "        df3.append(df2[df2.patID == pat_ID].values)\n",
    "        print(pat_ID)\n",
    "    for pat_ID in test_pat_list:\n",
    "        df4.append(df2[df2.patID == pat_ID].values) \n",
    "        print(pat_ID)\n",
    "    del df2\n",
    "    df3 = pd.np.vstack(df3)\n",
    "    df4 = pd.np.vstack(df4)\n",
    "    X_train = df3[0:, :3000]\n",
    "    X_test= df4[0:, :3000]\n",
    "    Y_train= df3[0:,3000:3001]\n",
    "    Y_test= df4[0:,3000:3001]\n",
    "    pat_train=df3[0:, 3002:3003]\n",
    "    pat_test= df4[0:, 3002:3003]\n",
    "    \n",
    "    del pd\n",
    "    return X_train,X_test,Y_train,Y_test,pat_train,pat_test\t\n",
    "\n",
    "\n",
    "\n",
    "df2 = pd.read_csv('E:/SleepWell/ASSC-master/data/purifiedallDataChannel2.csv',header=None)\n",
    "df2.rename({3000: 'hyp', 3001: 'epoch', 3002: 'patID'}, axis=\"columns\", inplace=True)\n",
    "\n",
    "\t#trainX, valX, trainY, valY = train_test_split(X, Y, test_size=0.2, random_state=random_seed) # patID split koro bacha\n",
    "trainX, valX, trainY, valY, pat_train, pat_val = patientSplitter('randomizedIDs.csv', df2, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72236, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4. 5. 6.]\n",
      "72236\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "num_samples = len(trainY)\n",
    "n_classes = len(np.unique(trainY))\n",
    "print(np.unique(trainY))\n",
    "print(num_samples)\n",
    "print(n_classes)\n",
    "#num_bin = np.bincount(Y[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dummytrainY= trainY.astype(int)\n",
    "dummytrainY= dummytrainY-1\n",
    "num_bin = np.bincount(trainY[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5],\n",
       "       [5],\n",
       "       [5],\n",
       "       ...,\n",
       "       [5],\n",
       "       [5],\n",
       "       [5]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72236\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(num_samples)\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
